{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torchvision import models\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from utils import *\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample= None, #PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "#         print(x.shape)\n",
    "#         x = self.patch_embed(x)\n",
    "#         print(\"Patch embedding: \", x.shape)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "#         print(\"Norm : \", x.shape)\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "#         print(\"AvgPool : \", x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        print(\"After forward features\", x.shape)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, img_size = 224, in_channels = 3):\n",
    "        super().__init__()\n",
    "        self.swin_transformer = SwinTransformer(img_size,in_chans = 3)\n",
    "        resnet = models.resnet18()\n",
    "        self.resnet_layers = list(resnet.children())[:8]\n",
    "        \n",
    "        self.p1_ch = nn.Conv2d(64, 96 , 1)\n",
    "        self.p1_pm = PatchMerging((56,56), 96)\n",
    "        \n",
    "        self.p2 = self.resnet_layers[5]\n",
    "        self.p2_ch = nn.Conv2d(64*2, 96*2 , 1)\n",
    "        self.p2_pm = PatchMerging((56 // 2,56 // 2), 96 * 2)\n",
    "        \n",
    "        \n",
    "        self.proj1_2 = nn.Linear(96, 192)\n",
    "        self.proj3_4 = nn.Linear(768, 384)\n",
    "#         self.cls_token_1_2 = nn.Parameter(torch.zeros(1,1,192))\n",
    "#         self.cls_token_3_4 = nn.Parameter(torch.zeros(1,1,384))\n",
    "        \n",
    "        \n",
    "        self.p3 = self.resnet_layers[6]\n",
    "        self.p3_ch = nn.Conv2d(64*4, 96*4 , 1)\n",
    "        self.p3_pm = PatchMerging((56 // 4,56 // 4), 96 * 4)\n",
    "        \n",
    "        \n",
    "        self.p4 = self.resnet_layers[7]\n",
    "        self.p4_ch = nn.Conv2d(64*8, 96*8 , 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(5):\n",
    "            x = self.resnet_layers[i](x) \n",
    "        \n",
    "        # 1\n",
    "        fm1 = x\n",
    "        fm1_ch = self.p1_ch(x)\n",
    "        B, C, H, W = fm1_ch.shape\n",
    "        fm1_reshaped = fm1_ch.view(B, C, W*H).permute(0,2,1)\n",
    "        sw1 = self.swin_transformer.layers[0](fm1_reshaped)\n",
    "        sw1_skipped = fm1_reshaped  + sw1\n",
    "        fm1_sw1 = self.p1_pm(sw1_skipped)\n",
    "#         print(fm1_sw1.shape)\n",
    "        \n",
    "        #2\n",
    "        fm1_sw2 = self.swin_transformer.layers[1](fm1_sw1)\n",
    "        fm2 = self.p2(fm1)\n",
    "        fm2_ch = self.p2_ch(fm2)\n",
    "        B, C, H, W = fm2_ch.shape\n",
    "        fm2_reshaped = fm2_ch.view(B, C, W*H).permute(0,2,1)\n",
    "        fm2_sw2_skipped = fm2_reshaped  + fm1_sw2\n",
    "        fm2_sw2 = self.p2_pm(fm2_sw2_skipped)\n",
    "#         print(fm2_sw2.shape)\n",
    "    \n",
    "        # Concat 1,2\n",
    "        \n",
    "        sw1_skipped_projected = self.proj1_2(sw1_skipped)\n",
    "        concat1 = torch.cat((sw1_skipped_projected, fm2_sw2_skipped), dim = 1)\n",
    "        \n",
    "#         print(concat1.shape)\n",
    "        \n",
    "        #3\n",
    "        fm2_sw3 = self.swin_transformer.layers[2](fm2_sw2)\n",
    "        fm3 = self.p3(fm2)\n",
    "        fm3_ch = self.p3_ch(fm3)\n",
    "        B, C, H, W = fm3_ch.shape\n",
    "        fm3_reshaped = fm3_ch.view(B, C, W*H).permute(0,2,1)\n",
    "        fm3_sw3_skipped = fm3_reshaped  + fm2_sw3\n",
    "        fm3_sw3 = self.p3_pm(fm3_sw3_skipped)\n",
    "#         print(fm3_sw3.shape)\n",
    "        \n",
    "        #4\n",
    "        fm3_sw4 = self.swin_transformer.layers[3](fm3_sw3)\n",
    "        fm4 = self.p4(fm3)\n",
    "        fm4_ch = self.p4_ch(fm4)\n",
    "        B, C, H, W = fm4_ch.shape\n",
    "        fm4_reshaped = fm4_ch.view(B, C, W*H).permute(0,2,1)\n",
    "        fm4_sw4_skipped = fm4_reshaped  + fm3_sw4\n",
    "#         print(fm4_sw4_skipped.shape)\n",
    "        \n",
    "        #concat 3,4\n",
    "        sw4_skipped_projected = self.proj3_4(fm4_sw4_skipped)\n",
    "        concat2 = torch.cat((sw4_skipped_projected, fm3_sw3_skipped), dim = 1)\n",
    "        \n",
    "        return concat1, concat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7679,  2.4407,  0.9987,  ...,  1.2072,  1.0333,  1.3805],\n",
       "          [-0.1800,  1.2878,  1.3751,  ...,  1.2845,  1.9379,  0.9335],\n",
       "          [ 0.8531,  1.6432,  2.3508,  ...,  1.1135,  1.7633, -0.6384],\n",
       "          ...,\n",
       "          [ 1.1367, -0.0928,  0.1825,  ...,  1.0492,  0.1088,  1.2998],\n",
       "          [-0.3801,  0.1943,  0.8767,  ..., -0.0459,  0.1252,  0.0906],\n",
       "          [ 1.9003, -0.4402,  0.5690,  ...,  1.1229,  0.3492,  1.0533]]],\n",
       "        grad_fn=<CatBackward>),\n",
       " tensor([[[-0.2945,  1.0569,  0.4859,  ...,  1.7706,  0.2262,  0.3331],\n",
       "          [ 0.5408,  0.5508,  0.5181,  ...,  1.4782,  0.3120,  0.5246],\n",
       "          [-0.1225,  1.1802, -0.0792,  ...,  0.6088,  0.4995,  0.3070],\n",
       "          ...,\n",
       "          [ 0.9302,  0.7493, -0.2047,  ..., -1.4375, -1.1442,  0.5552],\n",
       "          [ 1.1149, -0.3255,  0.0491,  ..., -0.4419,  1.0167, -0.1519],\n",
       "          [ 0.6931,  0.2816,  0.8239,  ..., -0.9740, -0.3825,  0.8078]]],\n",
       "        grad_fn=<CatBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyramidFeatures()(torch.rand(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Backup\n",
    "# class All2Cross(nn.Module):\n",
    "#     def __init__(self, img_size = 224, in_channels = 3):\n",
    "#         super().__init__()\n",
    "#         self.pyramid = PyramidFeatures(img_size= img_size, in_channels=in_channels)\n",
    "        \n",
    "#         self.attention1 = Attention(192)\n",
    "#         self.attention2 = Attention(384)\n",
    "        \n",
    "#         self.cls_token_1_2 = nn.Parameter(torch.zeros(1,1,192))\n",
    "#         self.cls_token_3_4 = nn.Parameter(torch.zeros(1,1,384))\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         concat1, concat2 = self.pyramid(x)\n",
    "        \n",
    "#         concat1 = torch.cat((concat1, self.cls_token_1_2), dim = 1)\n",
    "#         concat2 = torch.cat((concat2, self.cls_token_3_4), dim = 1)\n",
    "        \n",
    "#         attn1 = self.attention1(concat1)\n",
    "#         attn2 = self.attention2(concat2)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         return attn1, attn2\n",
    "#         print(attn1.shape)\n",
    "#         print(attn2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class All2Cross(nn.Module):\n",
    "    def __init__(self, img_size = 224, patch_size=(8, 16), in_chans=3, embed_dim=(192, 384),\n",
    "                 depth=([1, 3, 1], [1, 3, 1], [1, 3, 1]),\n",
    "                 num_heads=(6, 12), mlp_ratio=(2., 2., 4.), qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, multi_conv=False):\n",
    "        super().__init__()\n",
    "        self.pyramid = PyramidFeatures(img_size= img_size, in_channels=in_chans)\n",
    "        \n",
    "        self.attention1 = Attention(192)\n",
    "        self.attention2 = Attention(384)\n",
    "        \n",
    "        self.cls_token_1_2 = nn.Parameter(torch.zeros(1,1,192))\n",
    "        self.cls_token_3_4 = nn.Parameter(torch.zeros(1,1,384))\n",
    "        self.cls_token = nn.ParameterList([self.cls_token_1_2, self.cls_token_3_4])\n",
    "        \n",
    "        num_patches = (3920, 245) #Look at this later?!\n",
    "        self.num_branches = 2\n",
    "        \n",
    "        \n",
    "        total_depth = sum([sum(x[-2:]) for x in depth])\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule\n",
    "        dpr_ptr = 0\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for idx, block_cfg in enumerate(depth):\n",
    "            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n",
    "            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n",
    "            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr_,\n",
    "                                  norm_layer=norm_layer)\n",
    "            dpr_ptr += curr_depth\n",
    "            self.blocks.append(blk)\n",
    "\n",
    "        self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n",
    "#         self.head = nn.ModuleList([nn.Linear(embed_dim[i], num_classes) if num_classes > 0 else nn.Identity() for i in range(self.num_branches)])\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            trunc_normal_(self.cls_token[i], std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        concat1, concat2 = self.pyramid(x)\n",
    "        \n",
    "        concat1 = torch.cat((concat1, self.cls_token_1_2), dim = 1)\n",
    "        concat2 = torch.cat((concat2, self.cls_token_3_4), dim = 1)\n",
    "        \n",
    "        attn1 = self.attention1(concat1)\n",
    "        attn2 = self.attention2(concat2)\n",
    "        \n",
    "        xs = [attn1, attn2]\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            xs = blk(xs)\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = All2Cross()\n",
    "out = test(torch.rand(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 192])\n",
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "for o in out:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26734690\n",
      "40144962\n"
     ]
    }
   ],
   "source": [
    "print(get_n_params(PyramidFeatures().swin_transformer))\n",
    "print(get_n_params(PyramidFeatures()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55972866\n"
     ]
    }
   ],
   "source": [
    "print(get_n_params(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
