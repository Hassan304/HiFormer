{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db07dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torchvision import models\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from utils import *\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b809e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349adbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, factor, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim*factor),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2397f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3,\n",
    "                 embed_dim=96, depths=[2, 2, 6], num_heads=[3, 6, 12],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        patches_resolution = [img_size // patch_size, img_size // patch_size]\n",
    "        num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "        \n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample= None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58b9192a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=96, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(7, 7), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=96, window_size=(7, 7), num_heads=3\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=192, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=384, input_resolution=(14, 14), depth=6\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath()\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SwinTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f273d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipCLS(nn.Module):\n",
    "    def __init__(self, dim, factor):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.factor = factor\n",
    "        \n",
    "        self.multi_head_attention = Attention(dim=self.dim, factor=self.factor)\n",
    "        \n",
    "    def forward(self,embeddings, CLS):\n",
    "        in_MSA = torch.cat((CLS, embeddings), dim = 1)\n",
    "        MSA_out = self.multi_head_attention(in_MSA)\n",
    "        return MSA_out[:,0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e85933a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict\n",
    "\n",
    "# # # Resnet 18\n",
    "# cfg = config_dict.ConfigDict()\n",
    "# cfg.cnn_backbone = \"resnet18\"\n",
    "# cfg.cnn_pyramid_fm  = [64,128,256,512]\n",
    "# cfg.swin_pyramid_fm = [96, 192, 384, 768]\n",
    "# cfg.image_size = 224\n",
    "# cfg.patch_size = 4\n",
    "# cfg.num_classes = 1000\n",
    "\n",
    "# Resnet 50\n",
    "# cfg = config_dict.ConfigDict()\n",
    "# cfg.cnn_backbone = \"resnet50\"\n",
    "# cfg.cnn_pyramid_fm  = [256,512,1024,2048]\n",
    "# cfg.swin_pyramid_fm = [96, 192, 384, 768]\n",
    "# cfg.image_size = 224\n",
    "# cfg.patch_size = 4\n",
    "# cfg.num_classes = 1000\n",
    "\n",
    "# # # Resnet 34\n",
    "cfg = config_dict.ConfigDict()\n",
    "cfg.cnn_backbone = \"resnet34\"\n",
    "cfg.cnn_pyramid_fm  = [64,128,256,512]\n",
    "cfg.swin_pyramid_fm = [96, 192, 384, 768]\n",
    "cfg.image_size = 224\n",
    "cfg.patch_size = 4\n",
    "cfg.num_classes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a54413d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './weights/swin_tiny_patch4_window7_224.pth'\n",
    "swin_transformer = SwinTransformer(224,in_chans = 3)\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(device))['model']\n",
    "unexpected = [\"patch_embed.proj.weight\", \"patch_embed.proj.bias\", \"patch_embed.norm.weight\", \"patch_embed.norm.bias\",\n",
    "             \"head.weight\", \"head.bias\", \"layers.0.downsample.norm.weight\", \"layers.0.downsample.norm.bias\",\n",
    "             \"layers.0.downsample.reduction.weight\", \"layers.1.downsample.norm.weight\", \"layers.1.downsample.norm.bias\",\n",
    "             \"layers.1.downsample.reduction.weight\", \"layers.2.downsample.norm.weight\", \"layers.2.downsample.norm.bias\",\n",
    "             \"layers.2.downsample.reduction.weight\", \"norm.weight\", \"norm.bias\"]\n",
    "\n",
    "for key in list(checkpoint.keys()):\n",
    "    if key in unexpected or 'layers.3' in key:\n",
    "        del checkpoint[key]\n",
    "swin_transformer.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89bda1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec11f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, img_size = 224, in_channels = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.swin_transformer = SwinTransformer(img_size)\n",
    "        \n",
    "        model_path = './weights/swin_tiny_patch4_window7_224.pth'\n",
    "        self.swin_transformer = SwinTransformer(img_size,in_chans = 3)\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device(device))['model']\n",
    "        unexpected = [\"patch_embed.proj.weight\", \"patch_embed.proj.bias\", \"patch_embed.norm.weight\", \"patch_embed.norm.bias\",\n",
    "                     \"head.weight\", \"head.bias\", \"layers.0.downsample.norm.weight\", \"layers.0.downsample.norm.bias\",\n",
    "                     \"layers.0.downsample.reduction.weight\", \"layers.1.downsample.norm.weight\", \"layers.1.downsample.norm.bias\",\n",
    "                     \"layers.1.downsample.reduction.weight\", \"layers.2.downsample.norm.weight\", \"layers.2.downsample.norm.bias\",\n",
    "                     \"layers.2.downsample.reduction.weight\"]\n",
    "        for key in list(checkpoint.keys()):\n",
    "            if key in unexpected or 'layers.3' in key:\n",
    "                del checkpoint[key]\n",
    "        self.swin_transformer.load_state_dict(checkpoint)\n",
    "\n",
    "        \n",
    "        resnet = eval(f\"models.{cfg.cnn_backbone}()\")\n",
    "        self.resnet_layers = nn.ModuleList(resnet.children())[:7]\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,cfg.swin_pyramid_fm[0])) # Class Token\n",
    "\n",
    "        self.p1_ch = nn.Conv2d(cfg.cnn_pyramid_fm[0], cfg.swin_pyramid_fm[0] , kernel_size = 1)\n",
    "        self.p1_pm = PatchMerging((cfg.image_size // cfg.patch_size, cfg.image_size // cfg.patch_size), cfg.swin_pyramid_fm[0])\n",
    "        self.skip_CLS_1 = SkipCLS(dim=cfg.swin_pyramid_fm[0], factor=2)\n",
    "\n",
    "        self.p2 = self.resnet_layers[5]\n",
    "        self.p2_ch = nn.Conv2d(cfg.cnn_pyramid_fm[1], cfg.swin_pyramid_fm[1] , kernel_size = 1)\n",
    "        self.p2_pm = PatchMerging((cfg.image_size // cfg.patch_size // 2, cfg.image_size // cfg.patch_size // 2), cfg.swin_pyramid_fm[1])\n",
    "        self.skip_CLS_2 = SkipCLS(dim=cfg.swin_pyramid_fm[1], factor=2)\n",
    "        \n",
    "        self.p3 = self.resnet_layers[6]\n",
    "        self.p3_ch = nn.Conv2d(cfg.cnn_pyramid_fm[2] , cfg.swin_pyramid_fm[2] , kernel_size =  1)\n",
    "#         self.p3_pm = PatchMerging((cfg.image_size // cfg.patch_size // 4,cfg.image_size // cfg.patch_size // 4), cfg.swin_pyramid_fm[2])\n",
    "#         self.skip_CLS_3 = SkipCLS(dim=cfg.swin_pyramid_fm[2], factor=2)\n",
    "        \n",
    "#         self.p4 = self.resnet_layers[7]\n",
    "#         self.p4_ch = nn.Conv2d(cfg.cnn_pyramid_fm[3] , cfg.swin_pyramid_fm[3] , kernel_size = 1)\n",
    "        \n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(5):\n",
    "            x = self.resnet_layers[i](x) \n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        CLS = self.cls_token.expand(B, -1, -1)\n",
    "        sw1_CLS = CLS\n",
    "\n",
    "        # 1\n",
    "        fm1 = x\n",
    "        fm1_ch = self.p1_ch(x)\n",
    "        fm1_reshaped = Rearrange('b c h w -> b (h w) c')(fm1_ch)               \n",
    "        sw1 = self.swin_transformer.layers[0](fm1_reshaped)\n",
    "        sw1_skipped = fm1_reshaped  + sw1\n",
    "        CLS = self.skip_CLS_1(embeddings=sw1_skipped, CLS=CLS)\n",
    "        fm1_sw1 = self.p1_pm(sw1_skipped)\n",
    "        \n",
    "        #2\n",
    "        fm1_sw2 = self.swin_transformer.layers[1](fm1_sw1)\n",
    "        fm2 = self.p2(fm1)\n",
    "        fm2_ch = self.p2_ch(fm2)\n",
    "        fm2_reshaped = Rearrange('b c h w -> b (h w) c')(fm2_ch) \n",
    "        fm2_sw2_skipped = fm2_reshaped  + fm1_sw2\n",
    "        CLS = self.skip_CLS_2(embeddings=fm2_sw2_skipped, CLS=CLS)\n",
    "        fm2_sw2 = self.p2_pm(fm2_sw2_skipped)\n",
    "    \n",
    "        #3\n",
    "        fm2_sw3 = self.swin_transformer.layers[2](fm2_sw2)\n",
    "        fm3 = self.p3(fm2)\n",
    "        fm3_ch = self.p3_ch(fm3)\n",
    "        fm3_reshaped = Rearrange('b c h w -> b (h w) c')(fm3_ch) \n",
    "        fm3_sw3_skipped = fm3_reshaped  + fm2_sw3\n",
    "#         CLS = self.skip_CLS_3(embeddings=fm3_sw3_skipped, CLS=CLS)\n",
    "#         fm3_sw3 = self.p3_pm(fm3_sw3_skipped)\n",
    "        \n",
    "        #4\n",
    "#         fm3_sw4 = self.swin_transformer.layers[3](fm3_sw3)\n",
    "#         fm4 = self.p4(fm3)\n",
    "#         fm4_ch = self.p4_ch(fm4)\n",
    "#         fm4_reshaped = Rearrange('b c h w -> b (h w) c')(fm4_ch) \n",
    "#         fm4_sw4_skipped = fm4_reshaped  + fm3_sw4\n",
    "\n",
    "                \n",
    "        return [torch.cat((sw1_CLS, sw1_skipped), dim=1), torch.cat((CLS, fm3_sw3_skipped), dim=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2480207",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SwinTransformer:\n\tsize mismatch for norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-c1957051a362>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyramid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPyramidFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-caca64a51381>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, img_size, in_channels)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munexpected\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'layers.3'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswin_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1497\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1498\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1499\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SwinTransformer:\n\tsize mismatch for norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384])."
     ]
    }
   ],
   "source": [
    "pyramid = PyramidFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a119f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pyramid(torch.rand(2,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7d2a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class All2Cross(nn.Module):\n",
    "    def __init__(self, img_size = 224, in_chans=3, embed_dim=(96, 384),\n",
    "                 depth=[[1, 3, 1]],\n",
    "                 num_heads=(6, 12), mlp_ratio=(2., 2., 4.), qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, multi_conv=False):\n",
    "        super().__init__()\n",
    "        self.pyramid = PyramidFeatures(img_size= img_size, in_channels=in_chans)\n",
    "        \n",
    "        n_p1 = (cfg.image_size // cfg.patch_size) ** 2       # default: 3136 \n",
    "        n_p2 = (cfg.image_size // cfg.patch_size // 4) ** 2  # default: 49 \n",
    "        num_patches = (n_p1, n_p2)\n",
    "        \n",
    "        self.num_branches = 2\n",
    "        \n",
    "        \n",
    "        total_depth = sum([sum(x[-2:]) for x in depth])\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule\n",
    "        dpr_ptr = 0\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for idx, block_cfg in enumerate(depth):\n",
    "            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n",
    "            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n",
    "            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr_,\n",
    "                                  norm_layer=norm_layer)\n",
    "            dpr_ptr += curr_depth\n",
    "            self.blocks.append(blk)\n",
    "\n",
    "        self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xs = self.pyramid(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            xs = blk(xs)\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "        \n",
    "        return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c61dcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = All2Cross()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd7ec1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(torch.rand(1,3,224,224))[0].shape, test(torch.rand(1,3,224,224))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b57e0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1,49,768)  >>> (1,7*7,768)  >Permute> (1,768,7,7)     \n",
    "# (1,3136,96) >>> (1,56*56,96) >Permute> (1,96,56,56)\n",
    "\n",
    "# C1 = [96,56,56]  >conv -> [128,56,56] \n",
    "# C2 = [768,7,7]   >conv-2x -> [128,14,14] >conv-2x -> [128,28,28] >conv-2x -> [128,56,56]\n",
    "\n",
    "# C = C1 + C2 >>> [128,56,56]\n",
    "# [128,56,56] > Conv-4x -> [classes, 224,224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "306a11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvUpsample(nn.Module):\n",
    "    def __init__(self, in_chans=384, out_chans=[128], upsample=True):\n",
    "        super().__init__()\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        \n",
    "        self.conv_tower = nn.ModuleList()\n",
    "        for i, out_ch in enumerate(self.out_chans):\n",
    "            if i>0: self.in_chans = out_ch\n",
    "            self.conv_tower.append(nn.Conv2d(\n",
    "                self.in_chans, out_ch,\n",
    "                kernel_size=3, stride=1,\n",
    "                padding=1, bias=False\n",
    "            ))\n",
    "            self.conv_tower.append(nn.GroupNorm(32, out_ch))\n",
    "            self.conv_tower.append(nn.ReLU(inplace=False))\n",
    "            if upsample:\n",
    "                self.conv_tower.append(nn.Upsample(\n",
    "                        scale_factor=2, mode='bilinear', align_corners=False))\n",
    "            \n",
    "        self.convs_level = nn.Sequential(*self.conv_tower)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.convs_level(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8c794be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        super().__init__(conv2d)\n",
    "\n",
    "\n",
    "class SwinRetina_V2(nn.Module):\n",
    "    def __init__(self, img_size=224, in_chans=3, n_classes=9):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = [4, 16]\n",
    "        self.n_classes = n_classes\n",
    "        self.All2Cross = All2Cross( img_size= img_size, in_chans=in_chans)\n",
    "        \n",
    "        self.ConvUp_s = ConvUpsample(in_chans=384, out_chans=[128,128])\n",
    "        self.ConvUp_l = ConvUpsample(in_chans=96, out_chans=[128], upsample=False)\n",
    "        \n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=16,\n",
    "            out_channels=n_classes,\n",
    "            kernel_size=3,\n",
    "        )    \n",
    "\n",
    "        self.conv_pred = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                128, 16,\n",
    "                kernel_size=1, stride=1,\n",
    "                padding=0, bias=True),\n",
    "#             nn.GroupNorm(8, self.n_classes), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xs = self.All2Cross(x)\n",
    "        embeddings = [x[:, 1:] for x in xs]\n",
    "        reshaped_embed = []\n",
    "        for i, embed in enumerate(embeddings):\n",
    "\n",
    "            embed = Rearrange('b (h w) d -> b d h w', h=(self.img_size//self.patch_size[i]), w=(self.img_size//self.patch_size[i]))(embed)\n",
    "            embed = self.ConvUp_l(embed) if i == 0 else self.ConvUp_s(embed)\n",
    "            \n",
    "            reshaped_embed.append(embed)\n",
    "        C = reshaped_embed[0] + reshaped_embed[1]\n",
    "        C = self.conv_pred(C)\n",
    "\n",
    "        out = self.segmentation_head(C)\n",
    "        \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68957f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_swin_retina = SwinRetina_V2(n_classes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe27a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = test_swin_retina(torch.rand(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af6bb177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 224, 224])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c34b9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Swin: 11776170\n",
      "# Resnet: 8170304\n",
      "# SkipCLS + PM: 1238592\n",
      "# CrossViT: 5640672\n",
      "# Decoder: 704553\n",
      "Total: 26291699\n"
     ]
    }
   ],
   "source": [
    "swin_par = get_n_params(PyramidFeatures().swin_transformer)\n",
    "print('# Swin:', swin_par)\n",
    "\n",
    "res_par = get_n_params(PyramidFeatures().resnet_layers)\n",
    "remained_par = get_n_params(PyramidFeatures()) - swin_par - res_par\n",
    "print('# Resnet:', res_par)\n",
    "print('# SkipCLS + PM:', remained_par)\n",
    "\n",
    "\n",
    "crossvit_par = get_n_params(All2Cross()) - res_par - swin_par\n",
    "print('# CrossViT:', crossvit_par)\n",
    "\n",
    "decoder_par = get_n_params(SwinRetina_V2()) - crossvit_par - res_par - swin_par\n",
    "print('# Decoder:', decoder_par)\n",
    "\n",
    "print('Total:', get_n_params(SwinRetina_V2(n_classes=9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0676078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Swin: 18854826\n",
      "# Resnet: 21284672\n",
      "# SkipCLS + PM: 3799104\n",
      "# CrossViT: 5024160\n",
      "# Decoder: 631081\n",
      "Total: 45794739\n"
     ]
    }
   ],
   "source": [
    "swin_par = get_n_params(PyramidFeatures().swin_transformer)\n",
    "print('# Swin:', swin_par)\n",
    "\n",
    "res_par = get_n_params(PyramidFeatures().resnet_layers)\n",
    "remained_par = get_n_params(PyramidFeatures()) - swin_par - res_par\n",
    "print('# Resnet:', res_par)\n",
    "print('# SkipCLS + PM:', remained_par)\n",
    "\n",
    "\n",
    "crossvit_par = get_n_params(All2Cross()) - res_par - swin_par\n",
    "print('# CrossViT:', crossvit_par)\n",
    "\n",
    "decoder_par = get_n_params(SwinRetina_V2()) - crossvit_par - res_par - swin_par\n",
    "print('# Decoder:', decoder_par)\n",
    "\n",
    "print('Total:', swin_par + res_par + crossvit_par + decoder_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f38ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
